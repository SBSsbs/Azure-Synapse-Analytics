# Azure Synapse Analytics - Hands-on Labs
# LAB 01: Explorer Azure Synapse Analytics
**DurÃ©e: 1H**

Azure Synapse Analytics fournit une plateforme d'analyse de donnÃ©es unique et consolidÃ©e pour l'analyse de donnÃ©es de bout en bout. Dans cet exercice, vous allez explorer diffÃ©rentes maniÃ¨res d'ingÃ©rer et d'explorer des donnÃ©es. Cet exercice est conÃ§u comme une vue d'ensemble de haut niveau des diffÃ©rentes fonctionnalitÃ©s de base d'Azure Synapse Analytics. D'autres exercices sont disponibles pour explorer plus en dÃ©tail des fonctionnalitÃ©s spÃ©cifiques.

## Avant de commencer

Vous aurez besoin d'un abonnement Azure dans lequel vous disposez d'un accÃ¨s de niveau administratif.

## Provisionner un espace de travail Azure Synapse Analytics

Un espace de travail Azure Synapse Analytics fournit un point central pour la gestion des donnÃ©es et des runtimes de traitement des donnÃ©es. Vous pouvez provisionner un espace de travail Ã  l'aide de l'interface interactive du portail Azure, ou vous pouvez dÃ©ployer un espace de travail et les ressources qu'il contient Ã  l'aide d'un script ou d'un modÃ¨le. Dans la plupart des scÃ©narios de production, il est prÃ©fÃ©rable d'automatiser le provisionnement avec des scripts et des modÃ¨les afin de pouvoir intÃ©grer le dÃ©ploiement des ressources dans un processus de dÃ©veloppement et d'exploitation (DevOps) reproductible.

Dans cet exercice, vous utiliserez une combinaison d'un script PowerShell et d'un modÃ¨le ARM pour provisionner Azure Synapse Analytics.
1. Dans un navigateur Web, connectez-vous au portail Azure Ã  l'adresse https://portal.azure.com.
2. Utilisez le bouton [>_] Ã  droite de la barre de recherche en haut de la page pour crÃ©er un nouveau Cloud Shell dans le portail Azure, en sÃ©lectionnant un environnement PowerShell.
3. CrÃ©er un stockage si vous y Ãªtes invitÃ©. 
 
![Azure portal with a cloud shell pane](./images/001.png)

4. Le cloud shell fournit une interface de ligne de commande dans un volet au bas du portail Azure, comme illustrÃ© ici :
![Azure portal with a cloud shell pane](./images/002.png)

5. Si vous avez dÃ©jÃ  crÃ©Ã© un cloud shell qui utilise un environnement Bash, utilisez le menu dÃ©roulant en haut Ã  gauche du volet cloud shell pour le remplacer par PowerShell.
6. Notez que vous pouvez redimensionner le cloud shell en faisant glisser la barre de sÃ©paration en haut du volet ou en utilisant les icÃ´nes â€”, â—» et X en haut Ã  droite du volet pour rÃ©duire, agrandir et fermer le volet.
7. Dans le volet PowerShell, saisissez les commandes suivantes pour cloner ce rÃ©fÃ©rentiel :

```
rm -r synapse â€“f
git clone https://github.com/SBSsbs/Azure-Synapse-Analytics.git synapse
```
 
8. Une fois le rÃ©fÃ©rentiel clonÃ©, saisissez les commandes suivantes pour accÃ©der au dossier de cet exercice et exÃ©cutez le script setup.ps1 qu'il contient :

```
cd './synapse/Lab 01/01'
./setup.ps1
```

9. Si vous y Ãªtes invitÃ©, choisissez l'abonnement que vous souhaitez utiliser (cela ne se produira que si vous avez accÃ¨s Ã  plusieurs abonnements Azure).
10. Lorsque vous y Ãªtes invitÃ©, entrez un mot de passe appropriÃ© Ã  dÃ©finir pour votre pool SQL Azure Synapse.
11. Attendez que le script se termine - cela prend gÃ©nÃ©ralement environ 20 minutes, mais dans certains cas, cela peut prendre plus de temps.
DÃ©couvrez Synapse Studio
Synapse Studio est un portail Web dans lequel vous pouvez gÃ©rer et utiliser les ressources de votre espace de travail Azure Synapse Analytics.
12. Une fois l'exÃ©cution du script d'installation terminÃ©e, dans le portail Azure, accÃ©dez au groupe de ressources dp203-xxxxxxx qu'il a crÃ©Ã© et notez que ce groupe de ressources contient votre espace de travail Synapse.
 
13. Un compte de stockage pour votre lac de donnÃ©es, un pool Apache Spark (Apache Spark pool), un Pool d'explorateur de donnÃ©es (Data Explorer pool) et un pool SQL dÃ©diÃ© (Dedicated SQL pool).
 
14. SÃ©lectionnez votre espace de travail Synapse et, dans sa page Overview, dans la carte Open Synapse Studio, sÃ©lectionnez Open pour ouvrir Synapse Studio dans un nouvel onglet du navigateur. 
 
Synapse Studio est une interface Web que vous pouvez utiliser pour travailler avec votre espace de travail Synapse Analytics.
15. Sur le cÃ´tÃ© gauche de Synapse Studio, utilisez l'icÃ´ne â€ºâ€º pour dÃ©velopper le menu - cela rÃ©vÃ¨le les diffÃ©rentes pages de Synapse Studio que vous utiliserez pour gÃ©rer les ressources et effectuer des tÃ¢ches d'analyse de donnÃ©es, comme illustrÃ© ici :
 
16. Affichez la page DonnÃ©es (Data) et notez qu'il existe deux onglets contenant des sources de donnÃ©es :
â”€	Un onglet Workspace contenant les bases de donnÃ©es dÃ©finies dans l'espace de travail (y compris les bases de donnÃ©es SQL dÃ©diÃ©es et les bases de donnÃ©es Data Explorer).
â”€	Un onglet Linked contenant des sources de donnÃ©es liÃ©es Ã  l'espace de travail, y compris le stockage Azure Data Lake.
      
17. Affichez la page Develop. C'est ici que vous pouvez dÃ©finir des scripts et d'autres actifs utilisÃ©s pour dÃ©velopper des solutions de traitement de donnÃ©es.
18. Affichez la page Integrate. Vous utilisez cette page pour gÃ©rer les actifs d'ingestion et d'intÃ©gration de donnÃ©es ; tels que des pipelines pour transfÃ©rer et transformer des donnÃ©es entre des sources de donnÃ©es.
19. Affichez la page Monitor. C'est ici que vous pouvez observer les travaux de traitement de donnÃ©es pendant leur exÃ©cution et afficher leur historique.
20. Affichez la page Manage. C'est lÃ  que vous gÃ©rez les pools, les runtimes et les autres actifs utilisÃ©s dans votre espace de travail Azure Synapse. Voir chacun des onglets de la section Pools Analytics et notez que votre espace de travail comprend les pools suivants :
â”€	Pools SQL :
o	IntÃ©grÃ© : un pool SQL sans serveur que vous pouvez utiliser Ã  la demande pour explorer ou traiter des donnÃ©es dans un lac de donnÃ©es Ã  l'aide de commandes SQL.
o	sqlxxxxxxx : un pool SQL dÃ©diÃ© qui hÃ©berge une base de donnÃ©es d'entrepÃ´t de donnÃ©es relationnelles.
â”€	Groupes Apache Spark :
o	sparkxxxxxxx : que vous pouvez utiliser Ã  la demande pour explorer ou traiter des donnÃ©es dans un lac de donnÃ©es en utilisant des langages de programmation comme Scala ou Python.
â”€	Pools d'explorateur de donnÃ©es :
o	adxxxxxxxx : un pool d'explorateurs de donnÃ©es que vous pouvez utiliser pour analyser les donnÃ©es Ã  l'aide du langage de requÃªte Kusto (KQL).
 
IngÃ©rer des donnÃ©es avec un pipeline
L'une des principales tÃ¢ches que vous pouvez effectuer avec Azure Synapse Analytics consiste Ã  dÃ©finir des pipelines qui transfÃ¨rent (et si nÃ©cessaire, transforment) les donnÃ©es d'un large Ã©ventail de sources dans votre espace de travail pour analyse.
Utiliser la tÃ¢che Data Copy pour crÃ©er un pipeline
21. Dans Synapse Studio, sur la page Home, sÃ©lectionnez Ingest pour ouvrir l'outil Copy Data.
 
22. Dans l'outil Copy Data, Ã  l'Ã©tape PropriÃ©tÃ©s, assurez-vous que Built-in copy task et Run once now sont sÃ©lectionnÃ©s, puis cliquez sur Suivant >.
  
23. Ã€ l'Ã©tape Source, dans la sous-Ã©tape Dataset, sÃ©lectionnez les paramÃ¨tres suivants :
â”€	Type de source : Tous
â”€	Connexion : CrÃ©ez une nouvelle connexion et dans le volet linked services qui s'affiche, sous l'onglet Fichier, sÃ©lectionnez HTTP. 
 
24. Continuez ensuite et crÃ©ez une connexion Ã  un fichier de donnÃ©es en utilisant les paramÃ¨tres suivants :
â”€	Nom : Products
â”€	Description : liste de produits via http
â”€	Connectez-vous via le runtime d'intÃ©gration : AutoResolveIntegrationRuntime
â”€	URL de base : https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/01/adventureworks/products.csv
â”€	Validation du certificat du serveur : Activer
â”€	Type d'authentification : Anonyme
 
25. Avant de crÃ©er la connexion on peut la vÃ©rifier en utilisant le bouton Tester la connexion.
26. AprÃ¨s avoir crÃ©Ã© la connexion, sur la page Magasin de donnÃ©es source (Source data store), assurez-vous que les paramÃ¨tres suivants sont sÃ©lectionnÃ©s, puis sÃ©lectionnez Suivant > :
â”€	URL relative : laisser vide
â”€	MÃ©thode de requÃªte : GET
â”€	En-tÃªtes supplÃ©mentaires : laissez vide
â”€	Copie binaire : non sÃ©lectionnÃ©e
â”€	DÃ©lai d'expiration de la demande : laissez le champ vide
â”€	Nombre maximal de connexions simultanÃ©es : laissez le champ vide
27. Ã€ l'Ã©tape Source, dans la sous-Ã©tape Configuration, sÃ©lectionnez AperÃ§u des donnÃ©es pour afficher un aperÃ§u des donnÃ©es produit que votre pipeline va ingÃ©rer, puis fermez l'aperÃ§u.
 
28. AprÃ¨s avoir prÃ©-visualisÃ© les donnÃ©es, sur la page ParamÃ¨tres du format de fichier, assurez-vous que les paramÃ¨tres suivants sont sÃ©lectionnÃ©s, puis sÃ©lectionnez Suivant > :
â”€	Format de fichier : texte dÃ©limitÃ©
â”€	DÃ©limiteur de colonne : virgule (,)
â”€	DÃ©limiteur de ligne : saut de ligne (\n)
â”€	PremiÃ¨re ligne comme en-tÃªte : sÃ©lectionnÃ©
â”€	Type de compression : Aucun
29. Ã€ l'Ã©tape Destination, dans la sous-Ã©tape Dataset, sÃ©lectionnez les paramÃ¨tres suivants :
â”€	Type de destination : Azure Data Lake Storage Gen 2
â”€	Connexion : sÃ©lectionnez la connexion existante Ã  votre magasin de lac de donnÃ©es (celle-ci a Ã©tÃ© crÃ©Ã©e pour vous lorsque vous avez crÃ©Ã© l'espace de travail).
30. AprÃ¨s avoir sÃ©lectionnÃ© la connexion, Ã  l'Ã©tape Destination/Jeu de donnÃ©es, assurez-vous que les paramÃ¨tres suivants sont sÃ©lectionnÃ©s, puis sÃ©lectionnez Suivant > :
â”€	Chemin du dossier : files/product_data
â”€	Nom du fichier : produits.csv
â”€	Comportement de copie : aucun
â”€	Nombre maximal de connexions simultanÃ©es : laissez le champ vide
â”€	Taille du bloc (Mo) : Laisser vide
31. Ã€ l'Ã©tape Destination, Ã  la sous-Ã©tape Configuration, sur la page ParamÃ¨tres du format de fichier, assurez-vous que les propriÃ©tÃ©s suivantes sont sÃ©lectionnÃ©es. SÃ©lectionnez ensuite Suivant > :
â”€	Format de fichier : texte dÃ©limitÃ©
â”€	DÃ©limiteur de colonne : virgule (,)
â”€	DÃ©limiteur de ligne : saut de ligne (\n)
â”€	Ajouter un en-tÃªte au fichier : SÃ©lectionnÃ©
â”€	Type de compression : Aucun
â”€	Nombre maximum de lignes par fichier : laissez vide
â”€	PrÃ©fixe du nom de fichier : laissez vide
32. Ã€ l'Ã©tape ParamÃ¨tres, saisissez les paramÃ¨tres suivants, puis cliquez sur Suivant > :
â”€	Nom de la tÃ¢che : copier des produits
â”€	Description de la tÃ¢che : Copier les donnÃ©es des produits
â”€	TolÃ©rance aux pannes : laisser vide
â”€	Activer la journalisation : non sÃ©lectionnÃ©
â”€	Activer la mise en scÃ¨ne : non sÃ©lectionnÃ©
33. Ã€ l'Ã©tape RÃ©viser et terminer, Ã  la sous-Ã©tape RÃ©viser, lisez le rÃ©sumÃ©, puis cliquez sur Suivant >.
34. Ã€ l'Ã©tape DÃ©ploiement, attendez que le pipeline soit dÃ©ployÃ©, puis cliquez sur Terminer.
 
35. Dans Synapse Studio, sÃ©lectionnez la page Monitor, et dans l'onglet Pipeline runs, attendez que le pipeline Copy products se termine avec un statut RÃ©ussi (vous pouvez utiliser le bouton â†» Refresh sur la page Pipeline runs pour actualiser le statut).
36. Affichez la page IntÃ©grer et vÃ©rifiez qu'elle contient dÃ©sormais un pipeline nommÃ© Copier les produits.
 
Afficher les donnÃ©es ingÃ©rÃ©es
37. Sur la page DonnÃ©es, sÃ©lectionnez l'onglet Linked et dÃ©veloppez la hiÃ©rarchie Azure Data Lake Storage Fichiers produit jusqu'Ã  ce que vous voyiez le stockage des fichiers pour votre espace de travail Synapse. SÃ©lectionnez ensuite le stockage de fichiers pour vÃ©rifier qu'un dossier nommÃ© product_data contenant un fichier nommÃ© products.csv a Ã©tÃ© copiÃ© Ã  cet emplacement, comme illustrÃ© ici :
 
38. Cliquez avec le bouton droit sur le fichier de donnÃ©es products.csv et sÃ©lectionnez AperÃ§u pour visualiser les donnÃ©es ingÃ©rÃ©es. Fermez ensuite l'aperÃ§u.
 
Utiliser un pool SQL sans serveur pour analyser les donnÃ©es
Maintenant que vous avez ingÃ©rÃ© des donnÃ©es dans votre espace de travail, vous pouvez utiliser Synapse Analytics pour les interroger et les analyser. L'une des faÃ§ons les plus courantes d'interroger des donnÃ©es consiste Ã  utiliser SQL, et dans Synapse Analytics, vous pouvez utiliser un pool SQL sans serveur pour exÃ©cuter du code SQL sur des donnÃ©es dans un lac de donnÃ©es.
39. Dans Synapse Studio, cliquez avec le bouton droit sur le fichier products.csv dans le stockage de fichiers de votre espace de travail Synapse, pointez sur Nouveau script SQL et sÃ©lectionnez SÃ©lectionner les 100 premiÃ¨res lignes.
 
40. Dans le volet SQL Script 1 qui s'ouvre, passez en revue le code SQL qui a Ã©tÃ© gÃ©nÃ©rÃ©, qui devrait ressembler Ã  ceci :
-- This is auto-generated code
 SELECT
 â€¯â€¯â€¯â€¯TOPâ€¯100â€¯*
 FROM
 â€¯â€¯â€¯â€¯OPENROWSET(
â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯BULKâ€¯'https://datalakexxxxxxx.dfs.core.windows.net/files/product_data/products.csv',
 â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯FORMATâ€¯=â€¯'CSV',
 â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯PARSER_VERSION='2.0'
     )â€¯ASâ€¯[result]
41. Ce code ouvre un ensemble de lignes Ã  partir du fichier texte que vous avez importÃ© et rÃ©cupÃ¨re les 100 premiÃ¨res lignes de donnÃ©es.
42. Dans la liste Connect to (Se connecter Ã ), assurez-vous que Built-in est sÃ©lectionnÃ© - cela reprÃ©sente le pool SQL intÃ©grÃ© qui a Ã©tÃ© crÃ©Ã© avec votre espace de travail.
 
43. Dans la barre d'outils, utilisez le bouton â–· ExÃ©cuter pour exÃ©cuter le code SQL et examinez les rÃ©sultats, qui devraient ressembler Ã  ceci :
 
44. Notez que les rÃ©sultats se composent de quatre colonnes nommÃ©es C1, C2, C3 et C4 ; et que la premiÃ¨re ligne des rÃ©sultats contient les noms des champs de donnÃ©es. Pour rÃ©soudre ce problÃ¨me, ajoutez un paramÃ¨tre HEADER_ROW = TRUE Ã  la fonction OPENROWSET comme illustrÃ© ici (en remplaÃ§ant datalakexxxxxxx par le nom de votre compte de stockage de lac de donnÃ©es), puis relancez la requÃªte :
SELECT
 â€¯â€¯â€¯â€¯TOPâ€¯100â€¯*
 FROM
 â€¯â€¯â€¯â€¯OPENROWSET(
 â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯BULKâ€¯'https://datalakexxxxxxx.dfs.core.windows.net/files/product_data/products.csv',
 â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯FORMATâ€¯=â€¯'CSV',
 â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯PARSER_VERSION='2.0',
         HEADER_ROW = TRUE
     )â€¯ASâ€¯[result]
45. Maintenant, les rÃ©sultats ressemblent Ã  ceci :
 
46. Modifiez la requÃªte comme suit (en remplaÃ§ant datalakexxxxxxx par le nom de votre compte de stockage de lac de donnÃ©es) :
SELECT
 â€¯â€¯â€¯â€¯Category, COUNT(*) AS ProductCount
 FROM
 â€¯â€¯â€¯â€¯OPENROWSET(
 â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯BULKâ€¯'https://datalakexxxxxxx.dfs.core.windows.net/files/product_data/products.csv',
 â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯FORMATâ€¯=â€¯'CSV',
 â€¯â€¯â€¯â€¯â€¯â€¯â€¯â€¯PARSER_VERSION='2.0',
         HEADER_ROW = TRUE
     )â€¯ASâ€¯[result]
 GROUP BY Category;
47. ExÃ©cutez la requÃªte modifiÃ©e, qui devrait renvoyer un jeu de rÃ©sultats contenant le nombre de produits dans chaque catÃ©gorie, comme ceci :
 
48. Dans le volet PropriÃ©tÃ©s de SQL Script 1, remplacez le nom par Count Products by Category. 
49. Ensuite, dans la barre d'outils, sÃ©lectionnez Publier pour enregistrer le script.
50. Fermez le volet de script Count Products by Category.
51. Dans Synapse Studio, sÃ©lectionnez la page DÃ©velopper et notez que votre script SQL Count Products by Category publiÃ© y a Ã©tÃ© enregistrÃ©.
52. SÃ©lectionnez le script SQL Count Products by Category pour le rouvrir. Assurez-vous ensuite que le script est connectÃ© au pool SQL intÃ©grÃ© et exÃ©cutez-le pour rÃ©cupÃ©rer le nombre de produits.
53. Dans le volet RÃ©sultats, sÃ©lectionnez la vue Graphique, puis sÃ©lectionnez les paramÃ¨tres suivants pour le graphique :
â”€	Type de graphique : Colonne
â”€	Colonne CatÃ©gorie : CatÃ©gorie
â”€	Colonnes de lÃ©gende (sÃ©rie) : ProductCount
â”€	Position de la lÃ©gende : bas â€“ centre
â”€	Ã‰tiquette de lÃ©gende (sÃ©rie) : Laisser vide
â”€	LÃ©gende (sÃ©rie) valeur minimale : Laisser vide
â”€	LÃ©gende (sÃ©rie) maximum : Laisser vide
â”€	LibellÃ© de la catÃ©gorie : laisser vide
     Le graphique rÃ©sultant devrait ressembler Ã  ceci :
 
Utiliser un pool Spark pour analyser les donnÃ©es
Alors que SQL est un langage courant pour interroger des ensembles de donnÃ©es structurÃ©s, de nombreux analystes de donnÃ©es trouvent des langages comme Python utiles pour explorer et prÃ©parer les donnÃ©es pour l'analyse. Dans Azure Synapse Analytics, vous pouvez exÃ©cuter du code Python (et autre) dans un pool Spark ; qui utilise un moteur de traitement de donnÃ©es distribuÃ© basÃ© sur Apache Spark.
54. Dans Synapse Studio, si l'onglet Fichiers que vous avez ouvert prÃ©cÃ©demment contenant le fichier products.csv n'est plus ouvert, sur la page DonnÃ©es, parcourez le dossier product_data. 
55. Cliquez ensuite avec le bouton droit sur products.csv, pointez sur Nouveau bloc-notes et sÃ©lectionnez Charger dans DataFrame.
 
56. Dans le volet Bloc-notes 1 qui s'ouvre, dans la liste Attacher Ã , sÃ©lectionnez le pool Spark xxxxxxxx et assurez-vous que la langue est dÃ©finie sur PySpark (Python).
 
57. Passez en revue le code dans la premiÃ¨re (et unique) cellule du bloc-notes, qui devrait ressembler Ã  ceci :
%%pyspark
df = spark.read.load('abfss://files@datalakeex24n6q.dfs.core.windows.net/product_data/produits.csv', format='csv'
## Ifâ€¯headerâ€¯existsâ€¯uncommentâ€¯lineâ€¯below
##, header=True
)
display(df.limit(10))
58. Utilisez l'icÃ´ne â–· Ã  gauche pour l'exÃ©cuter, et attendez les rÃ©sultats. 
59. La premiÃ¨re fois que vous exÃ©cutez une cellule dans un bloc-notes, le pool Spark est dÃ©marrÃ©. Cela peut donc prendre environ une minute pour renvoyer des rÃ©sultats.
60. Finalement, les rÃ©sultats devraient apparaÃ®tre sous la cellule, et ils devraient ressembler Ã  ceci :
 
61. DÃ©-commentez la ligne, header=True (car le fichier products.csv a les en-tÃªtes de colonne sur la premiÃ¨re ligne).
62. RÃ©-exÃ©cutez la cellule et vÃ©rifiez que les rÃ©sultats ressemblent Ã  ceci :
 
63. Notez que la rÃ©exÃ©cution de la cellule prend moins de temps, car le pool Spark est dÃ©jÃ  dÃ©marrÃ©.
64. Sous les rÃ©sultats, utilisez l'icÃ´ne ï¼‹ Code pour ajouter une nouvelle cellule de code au bloc-notes.
65. Dans la nouvelle cellule de code vide, ajoutez le code suivant :
      df_counts = df.groupby(df.Category).count()
      display(df_counts)
66. ExÃ©cutez la nouvelle cellule de code en cliquant sur son icÃ´ne â–· et examinez les rÃ©sultats, qui devraient ressembler Ã  ceci :
 
67. Dans la sortie des rÃ©sultats de la cellule, sÃ©lectionnez-la vue Graphique. Le graphique rÃ©sultant devrait ressembler Ã  ceci :
 
68. Si elle n'est pas dÃ©jÃ  visible, affichez la page PropriÃ©tÃ©s en sÃ©lectionnant le bouton PropriÃ©tÃ©s Ã  l'extrÃ©mitÃ© droite de la barre d'outils. 
69. Ensuite, dans le volet PropriÃ©tÃ©s, modifiez le nom du bloc-notes en Explorer les produits et utilisez le bouton Publier de la barre d'outils pour l'enregistrer.
70. Fermez le volet du bloc-notes et arrÃªtez la session Spark lorsque vous y Ãªtes invitÃ©. Affichez ensuite la page DÃ©velopper pour vÃ©rifier que le bloc-notes a Ã©tÃ© enregistrÃ©.
Utiliser un pool SQL dÃ©diÃ© pour interroger un entrepÃ´t de donnÃ©es
Jusqu'Ã  prÃ©sent, vous avez vu quelques techniques d'exploration et de traitement de donnÃ©es basÃ©es sur des fichiers dans un lac de donnÃ©es. Dans de nombreux cas, une solution d'analyse d'entreprise utilise un lac de donnÃ©es pour stocker et prÃ©parer des donnÃ©es non structurÃ©es qui peuvent ensuite Ãªtre chargÃ©es dans un entrepÃ´t de donnÃ©es relationnelles pour prendre en charge les charges de travail de Business Intelligence (BI). Dans Azure Synapse Analytics, ces entrepÃ´ts de donnÃ©es peuvent Ãªtre implÃ©mentÃ©s dans un pool SQL dÃ©diÃ©.
71. Dans Synapse Studio, sur la page Manage, dans la section Pools SQL, sÃ©lectionnez la ligne du pool SQL dÃ©diÃ© sqlxxxxxxx
72. Utilisez son icÃ´ne â–· pour le reprendre.
 
73. Attendez que le pool SQL dÃ©marre. Cela peut prendre quelques minutes. Utilisez le bouton â†» Actualiser pour vÃ©rifier pÃ©riodiquement son Ã©tat. L'Ã©tat s'affichera comme Online lorsqu'il sera prÃªt.
74. Lorsque le pool SQL a dÃ©marrÃ©, sÃ©lectionnez la page Data.
75. Dans l'onglet Espace de travail, dÃ©veloppez les bases de donnÃ©es SQL et vÃ©rifiez que sqlxxxxxxx est rÃ©pertoriÃ© (utilisez l'icÃ´ne â†» en haut Ã  gauche de la page pour actualiser la vue si nÃ©cessaire).
 
76. DÃ©veloppez la base de donnÃ©es sqlxxxxxxx et son dossier Tables, puis dans le menu â€¦ de la table FactInternetSales, pointez sur Nouveau script SQL et sÃ©lectionnez SÃ©lectionner les 100 premiÃ¨res lignes.
 
77. Examinez les rÃ©sultats de la requÃªte, qui affichent les 100 premiÃ¨res transactions de vente dans le tableau. Ces donnÃ©es ont Ã©tÃ© chargÃ©es dans la base de donnÃ©es par le script d'installation et sont stockÃ©es en permanence dans la base de donnÃ©es associÃ©e au pool SQL dÃ©diÃ©.
78. Remplacez la requÃªte SQL par le code suivant :
     SELECT d.CalendarYear, d.MonthNumberOfYear, d.EnglishMonthName,
        p.EnglishProductName AS Product, SUM(o.OrderQuantity) AS UnitsSold
 FROM dbo.FactInternetSales AS o
 JOIN dbo.DimDate AS d ON o.OrderDateKey = d.DateKey
 JOIN dbo.DimProduct AS p ON o.ProductKey = p.ProductKey
 GROUP BY d.CalendarYear, d.MonthNumberOfYear, d.EnglishMonthName, p.EnglishProductName
 ORDER BY d.MonthNumberOfYear
79. Utilisez le bouton â–· ExÃ©cuter pour exÃ©cuter la requÃªte modifiÃ©e, qui renvoie la quantitÃ© de chaque produit vendu par annÃ©e et par mois.
 
80. Si elle n'est pas dÃ©jÃ  visible, affichez la page PropriÃ©tÃ©s en sÃ©lectionnant le bouton PropriÃ©tÃ©s Ã  l'extrÃ©mitÃ© droite de la barre d'outils. 
81. Ensuite, dans le volet PropriÃ©tÃ©s, modifiez le nom de la requÃªte en Aggregate product sales.
82. Utilisez le bouton Publier de la barre d'outils pour l'enregistrer.
83. Fermez le volet de requÃªte, puis affichez la page Develop pour vÃ©rifier que le script SQL a Ã©tÃ© enregistrÃ©.
84. Sur la page Manage, sÃ©lectionnez la ligne du pool SQL dÃ©diÃ© sqlxxxxxxx et utilisez son icÃ´ne âšâš pour la mettre en pause.
Explorer les donnÃ©es avec un pool d'explorateur de donnÃ©es
Azure Synapse Data Explorer fournit un environnement d'exÃ©cution que vous pouvez utiliser pour stocker et interroger des donnÃ©es Ã  l'aide du langage de requÃªte Kusto (KQL). Kusto est optimisÃ© pour les donnÃ©es qui incluent un composant de sÃ©rie chronologique, telles que les donnÃ©es en temps rÃ©el provenant de fichiers journaux ou d'appareils IoT.
CrÃ©er une base de donnÃ©es Data Explorer et ingÃ©rer des donnÃ©es dans une table
85. Dans Synapse Studio, sur la page Manage, dans le pool Data Explorer pools, sÃ©lectionnez la ligne du pool adxxxxxxxx, puis utilisez son icÃ´ne â–· pour la reprendre.
86. Attendez que le pool dÃ©marre. Cela peut prendre un certain temps. Utilisez le bouton â†» Actualiser pour vÃ©rifier pÃ©riodiquement son Ã©tat. Le statut s'affichera comme en ligne lorsqu'il sera prÃªt.
87. Lorsque le pool de l'explorateur de donnÃ©es a dÃ©marrÃ©, affichez la page Data; et dans l'onglet Espace de travail, dÃ©veloppez Bases de donnÃ©es de l'explorateur de donnÃ©es et vÃ©rifiez que adxxxxxxxx est rÃ©pertoriÃ© (utilisez l'icÃ´ne â†» en haut Ã  gauche de la page pour actualiser la vue si nÃ©cessaire)
88. Dans le volet Data, utilisez l'icÃ´ne ï¼‹ pour crÃ©er une nouvelle base de donnÃ©es Data Explorer dans le pool adxxxxxxxx avec le nom sales-data.
 
89. Dans Synapse Studio, attendez que la base de donnÃ©es soit crÃ©Ã©e (une notification s'affichera).
90. Passez Ã  la page Develop et, dans la liste des scripts KQL, sÃ©lectionnez ingest-data. Lorsque le script s'ouvre, notez qu'il contient deux instructions :
â”€	Une instruction .create table pour crÃ©er une table nommÃ©e sales.
â”€	Une instruction .ingest into table pour charger des donnÃ©es dans la table Ã  partir d'une source HTTP.
91. Dans le volet ingest-data, dans la liste Connect to, sÃ©lectionnez votre pool adxxxxxxxx, et dans la liste Database, sÃ©lectionnez sales-data.
 
92. Dans le script, mettez en surbrillance l'instruction .create table, puis dans la barre d'outils, utilisez le bouton â–· Run pour exÃ©cuter le code sÃ©lectionnÃ©, ce qui crÃ©e une table nommÃ©e sales.
93. Une fois la table crÃ©Ã©e, mettez en surbrillance l'instruction .ingest into table et utilisez le bouton â–· Run pour l'exÃ©cuter et ingÃ©rer les donnÃ©es dans la table.
94. Dans cet exemple, vous avez importÃ© une trÃ¨s petite quantitÃ© de donnÃ©es de lot Ã  partir d'un fichier, ce qui convient aux fins de cet exercice. En rÃ©alitÃ©, vous pouvez utiliser Data Explorer pour analyser des volumes de donnÃ©es beaucoup plus importants ; y compris les donnÃ©es en temps rÃ©el d'une source de diffusion telle qu'Azure Event Hubs.
Utiliser le langage de requÃªte Kusto pour interroger la table
95. Revenez Ã  la page Data et dans le menu â€¦ de la base de donnÃ©es des donnÃ©es sales-data, sÃ©lectionnez Actualiser.
 
96. DÃ©veloppez le dossier Tables de la base de donnÃ©es des donnÃ©es sales-data. 
97. Ensuite, dans le menu â€¦ de la table des ventes, sÃ©lectionnez Nouveau script KQL > Prendre 1000 lignes.
 
98. Passez en revue la requÃªte gÃ©nÃ©rÃ©e et ses rÃ©sultats. La requÃªte doit contenir le code suivant :
     sales
 | take 1000
99. Les rÃ©sultats de la requÃªte contiennent les 1 000 premiÃ¨res lignes de donnÃ©es.
 
100. Modifiez la requÃªte comme suit :
sales
 | where Item == 'Road-250 Black, 48'
101.  Utilisez le bouton â–· ExÃ©cuter pour exÃ©cuter la requÃªte. 
102. Examinez ensuite les rÃ©sultats, qui ne doivent contenir que les lignes des bons de commande pour le produit Road-250 Black, 48.
103. Modifiez la requÃªte comme suit :
sales
 | where Item == 'Road-250 Black, 48'
 | where datetime_part('year', OrderDate) > 2020
104.  ExÃ©cutez la requÃªte et examinez les rÃ©sultats, qui ne doivent contenir que des commandes client pour Road-250 Black, 48 effectuÃ©es aprÃ¨s 2020.
105. Modifiez la requÃªte comme suit :
sales
 | where OrderDate between (datetime(2020-01-01 00:00:00) .. datetime(2020-12-31 23:59:59))
 | summarize TotalNetRevenue = sum(UnitPrice) by Item
 | sort by Item asc
106. ExÃ©cutez la requÃªte et examinez les rÃ©sultats, qui doivent contenir le revenu net total pour chaque produit entre le 1er janvier et le 31 dÃ©cembre 2020, par ordre croissant de nom de produit.
107. Si elle n'est pas dÃ©jÃ  visible, affichez la page PropriÃ©tÃ©s en sÃ©lectionnant le bouton PropriÃ©tÃ©s Ã  l'extrÃ©mitÃ© droite de la barre d'outils. 
108. Ensuite, dans le volet PropriÃ©tÃ©s, modifiez le nom de la requÃªte en Explore sales data de vente et utilisez le bouton Publier de la barre d'outils pour l'enregistrer.
109. Fermez le volet de requÃªte, puis affichez la page Develop pour vÃ©rifier que le script KQL a Ã©tÃ© enregistrÃ©.
110. Sur la page Manage, sÃ©lectionnez la ligne du pool adxxxxxxxx Data Explorer et utilisez son icÃ´ne âšâš pour la mettre en pause.
Supprimer des ressources Azure
Maintenant que vous avez fini d'explorer Azure Synapse Analytics, vous devez supprimer les ressources que vous avez crÃ©Ã©es pour Ã©viter des coÃ»ts Azure inutiles.
111. Fermez l'onglet du navigateur Synapse Studio et revenez au portail Azure.
112. Sur le portail Azure, sur la page d'accueil, sÃ©lectionnez Groupes de ressources.
113. SÃ©lectionnez le groupe de ressources dp203-xxxxxxx pour votre espace de travail Synapse Analytics (pas le groupe de ressources gÃ©rÃ©es) et vÃ©rifiez qu'il contient l'espace de travail Synapse, le compte de stockage, le pool SQL, le pool Data Explorer et le pool Spark pour votre espace de travail.
114. En haut de la page PrÃ©sentation de votre groupe de ressources, sÃ©lectionnez Supprimer le groupe de ressources.
115. Entrez le nom du groupe de ressources dp203-xxxxxxx pour confirmer que vous souhaitez le supprimer, puis sÃ©lectionnez Supprimer.
116. AprÃ¨s quelques minutes, votre groupe de ressources d'espace de travail Azure Synapse et le groupe de ressources d'espace de travail gÃ©rÃ© qui lui est associÃ© seront supprimÃ©s.


## PrÃ©requis

Avant de commencer les exercices, assurez-vous dâ€™avoir :

- Un abonnement Azure actif (idÃ©alement avec des crÃ©dits de formation ou un abonnement payant)
- Les droits nÃ©cessaires pour crÃ©er des ressources dans une ressource group (Contributor au minimum)
- Un ordinateur avec :
  - Un navigateur web rÃ©cent (Edge ou Chrome recommandÃ©)
  - [Azure Data Studio](https://learn.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio) (optionnel mais recommandÃ© pour certains labs)
  - [Azure Storage Explorer](https://azure.microsoft.com/en-us/features/storage-explorer/) (optionnel)

> **Note** : Les labs utilisent souvent un environnement temporaire (ex. : sandbox Microsoft Learn ou dÃ©ploiement via script). Si vous travaillez en autonomie, vous devrez crÃ©er votre propre ressource Synapse Workspace.

## Structure du repository

Chaque dossier de lab contient :
- Un fichier `README.md` avec les instructions dÃ©taillÃ©es de lâ€™exercice
- Les notebooks Synapse (.ipynb)
- Les scripts SQL
- Les pipelines (fichiers .json si exportÃ©s)
- Les jeux de donnÃ©es dâ€™exemple

## Comment dÃ©marrer les labs

### Option 1 : Pendant la formation (recommandÃ©)
Le formateur vous fournira :
- Lâ€™accÃ¨s Ã  un environnement prÃ©-dÃ©ployÃ© (Synapse Workspace, donnÃ©es chargÃ©es, etc.)
- Les instructions spÃ©cifiques pour chaque lab

Suivez simplement les README dans chaque dossier de lab.

### Option 2 : En autonomie aprÃ¨s la formation

1. CrÃ©ez un Azure Synapse Workspace dans votre abonnement
2. TÃ©lÃ©chargez ou clonez ce repository
3. Ouvrez le workspace Synapse Studio
4. Importez les notebooks, pipelines et scripts SQL depuis les dossiers correspondants
5. Chargez les donnÃ©es dâ€™exemple depuis le dossier `resources/`
6. Suivez les instructions dans le `README.md` de chaque lab

> **Astuce** : Certains labs incluent des scripts de dÃ©ploiement automatisÃ© (ARM templates ou PowerShell). VÃ©rifiez dans le dossier `scripts/` si disponible.

## Nettoyage des ressources

Pour Ã©viter des coÃ»ts inutiles :
- Supprimez le resource group contenant votre Synapse Workspace Ã  la fin des exercices
- Ou mettez en pause les pools Spark / SQL dÃ©diÃ©s

## Support et questions

- Si vous rencontrez un problÃ¨me avec les instructions ou les fichiers â†’ ouvrez une **Issue** sur ce repository
- Pour des questions sur Azure Synapse Analytics â†’ consultez la [documentation officielle Microsoft](https://learn.microsoft.com/en-us/azure/synapse-analytics/)

## Contributeurs

- [Votre Nom] â€“ Formateur principal / CrÃ©ateur du contenu

---

Merci dâ€™avoir suivi cette formation !  
Bonne pratique avec Azure Synapse Analytics ðŸš€
