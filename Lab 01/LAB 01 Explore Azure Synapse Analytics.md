# Azure Synapse Analytics - Hands-on Labs
# LAB 01: Explorer Azure Synapse Analytics
**Dur√©e: 1H**

Azure Synapse Analytics fournit une plateforme d'analyse de donn√©es unique et consolid√©e pour l'analyse de donn√©es de bout en bout. Dans cet exercice, vous allez explorer diff√©rentes mani√®res d'ing√©rer et d'explorer des donn√©es. Cet exercice est con√ßu comme une vue d'ensemble de haut niveau des diff√©rentes fonctionnalit√©s de base d'Azure Synapse Analytics. D'autres exercices sont disponibles pour explorer plus en d√©tail des fonctionnalit√©s sp√©cifiques.

## Avant de commencer

Vous aurez besoin d'un abonnement Azure dans lequel vous disposez d'un acc√®s de niveau administratif.

## Provisionner un espace de travail Azure Synapse Analytics

Un espace de travail Azure Synapse Analytics fournit un point central pour la gestion des donn√©es et des runtimes de traitement des donn√©es. Vous pouvez provisionner un espace de travail √† l'aide de l'interface interactive du portail Azure, ou vous pouvez d√©ployer un espace de travail et les ressources qu'il contient √† l'aide d'un script ou d'un mod√®le. Dans la plupart des sc√©narios de production, il est pr√©f√©rable d'automatiser le provisionnement avec des scripts et des mod√®les afin de pouvoir int√©grer le d√©ploiement des ressources dans un processus de d√©veloppement et d'exploitation (DevOps) reproductible.

Dans cet exercice, vous utiliserez une combinaison d'un script PowerShell et d'un mod√®le ARM pour provisionner Azure Synapse Analytics.
1. Dans un navigateur Web, connectez-vous au portail Azure √† l'adresse https://portal.azure.com.
2. Utilisez le bouton [>_] √† droite de la barre de recherche en haut de la page pour cr√©er un nouveau Cloud Shell dans le portail Azure, en s√©lectionnant un environnement PowerShell.
3. Cr√©er un stockage si vous y √™tes invit√©. 
 
![Azure portal with a cloud shell pane](./images/001.png)

4. Le cloud shell fournit une interface de ligne de commande dans un volet au bas du portail Azure, comme illustr√© ici :
![Azure portal with a cloud shell pane](./images/002.png)

5. Si vous avez d√©j√† cr√©√© un cloud shell qui utilise un environnement Bash, utilisez le menu d√©roulant en haut √† gauche du volet cloud shell pour le remplacer par PowerShell.
6. Notez que vous pouvez redimensionner le cloud shell en faisant glisser la barre de s√©paration en haut du volet ou en utilisant les ic√¥nes ‚Äî, ‚óª et X en haut √† droite du volet pour r√©duire, agrandir et fermer le volet.
7. Dans le volet PowerShell, saisissez les commandes suivantes pour cloner ce r√©f√©rentiel :

```
rm -r synapse ‚Äìf
git clone https://github.com/SBSsbs/Azure-Synapse-Analytics.git synapse
```
 
8. Une fois le r√©f√©rentiel clon√©, saisissez les commandes suivantes pour acc√©der au dossier de cet exercice et ex√©cutez le script setup.ps1 qu'il contient :

```
cd './synapse/Lab 01/01'
./setup.ps1
```

9. Si vous y √™tes invit√©, choisissez l'abonnement que vous souhaitez utiliser (cela ne se produira que si vous avez acc√®s √† plusieurs abonnements Azure).
10. Lorsque vous y √™tes invit√©, entrez un mot de passe appropri√© √† d√©finir pour votre pool SQL Azure Synapse.
11. Attendez que le script se termine - cela prend g√©n√©ralement environ 20 minutes, mais dans certains cas, cela peut prendre plus de temps.

Dans cette partie du lab, nous allons provisionner automatiquement l‚Äôensemble de l‚Äôenvironnement Azure Synapse Analytics qui sera utilis√© tout au long de l'exercie. Cette t√¢che consiste √† ex√©cuter un script PowerShell qui d√©ploie, via de l‚ÄôInfrastructure as Code, toutes les ressources n√©cessaires √† un sc√©nario Data Engineering complet : un **Data Lake Azure (ADLS Gen2)** pour le stockage des donn√©es, un **workspace Synapse** servant de point central d‚Äôorchestration, un **Spark pool** pour les traitements distribu√©s, ainsi qu‚Äôun **Dedicated SQL pool** jouant le r√¥le de couche Data Warehouse. 

Le script configure √©galement les droits d‚Äôacc√®s (**RBAC**) entre les services, initialise le **sch√©ma analytique** (tables de faits et de dimensions) et charge des jeux de donn√©es de r√©f√©rence. 

L‚Äôobjectif n‚Äôest pas seulement de disposer d‚Äôun environnement pr√™t √† l‚Äôemploi, mais surtout de comprendre ce qui est d√©ploy√© en arri√®re-plan, comment les diff√©rents composants interagissent, et pourquoi ces choix correspondent √† une architecture Data Engineering moderne et industrialisable.

## 1. Mise en place de l‚Äôenvironnement du lab avec le script PowerShell##


``` Clear-Host ```: cette action permet de nettoyer la console et donc am√©liorer la lisibilit√© pendant une d√©mo/lab.

```write-host "Starting script at $(Get-Date)"```: il s'agit d'un log de d√©marrage avec horodatage pour permettre la tra√ßabilit√© (utile quand plusieurs participants ex√©cutent le setup).

```Set-PSRepository -Name PSGallery -InstallationPolicy Trusted```: d√©finit PSGallery comme ‚ÄúTrusted‚Äù ce qui √©vite les prompts interactifs au moment d‚Äôinstaller des modules ‚Üí ex√©cution fluide en salle.

```Install-Module -Name Az.Synapse -Force```: permet d'installer le module PowerShell Az.Synapse. Il s'agit d'un module client pour piloter Synapse via PowerShell ce qui illustre l‚Äôautomatisation (scripts/IaC) : on ne clique pas dans le portail, on industrialise.

### 2. Gestion du contexte Azure (abonnement)##

``` Get-AzSubscription```: Cette commande permet de lister les subscriptions accessibles par l‚Äôutilisateur. Dans un contexte de formation, certains participants peuvent avoir plusieurs abonnements (perso + pro + sandbox) surtout si aucun abonnement n'est propos dans le cadre de la formation. Ceci permet d'√©viter de d√©ployer ‚Äúau mauvais endroit‚Äù. On ajoute alors √† notre code PowerShell, un bloc de s√©lection qui permet de choisir la subscription √† utiliser. Le script va alors afficher la liste (```Name```, ```Id```), demande √† l'utilisateur de s√©lectionner un index et le lire comme argument en utilisant la commande ```Read-Host```, le valide s'il est correcte puis applique le contexte en utilisant les instructions suivantes: 

```
Select-AzSubscription -SubscriptionId $selectedSub
az account set --subscription $selectedSub
```
## 3. Saisie et validation du mot de passe SQL##

Il s'agit d'une boucle de saisie o√π le script impose d√©j√† un nom d'utilisateur ```$sqlUser = "SQLUser" ``` demande un mot de passe ```$sqlPassword``` avec une complexit√© : min 8 caract√®res, 1 majuscule, 1 minuscule, 1 chiffre et 1 caract√®re sp√©cial parmi ! @ # % ^ & $. Ce login/mot de passe sert ensuite √† se connecter au endpoint SQL de Synapse (Dedicated SQL pool / SQL database dans le workspace). Ceci permet de manipuler SQL, charger des donn√©es et ex√©cuter des scripts sans d√©pendre d‚ÄôAzure AD au d√©part.

## 4. Enregistrement des Resource Providers Azure##

```
$provider_list = "Microsoft.Synapse", "Microsoft.Sql", "Microsoft.Storage", "Microsoft.Compute"
foreach ($provider in $provider_list){
    Register-AzResourceProvider -ProviderNamespace $provider
}
```

Ces instructions permettent d'activer les providers dans la subscription si n√©cessaire. Ceci permet d'√©viter les √©checs de d√©ploiement ARM/Bicep du type ‚ÄúThe subscription is not registered to use namespace ‚Ä¶‚Äù ce qui est important lorsqu'il s'agit de manipuler en environnement ‚Äúneuf‚Äù (sandbox formation, nouvel abonnement)

## 5. G√©n√©ration d‚Äôun suffixe unique pour nommer les ressources##
```
[string]$suffix = -join ((48..57) + (97..122) | Get-Random -Count 7 | % {[char]$_})
$resourceGroupName = "semeh-$suffix"
```

Ces instructions g√©n√®rent un suffixe al√©atoire (chiffres + lettres minuscules). Avec Azure, beaucoup de noms doivent √™tre globalement uniques (ex: storage account). Dans un contexte de formation, 20 personnes qui cr√©ent ‚Äúdatalake‚Äù sur le m√™me abonnement peut engendrer des collisions assur√©es. Le suffixe √©vite √ßa.

Ressources nomm√©es (plus bas) :

- RG : semeh-<suffix>
- Synapse : synapse<suffix>
- Storage : datalake<suffix>
- Spark pool : spark<suffix>
- SQL database/pool : sql<suffix>

## 6. S√©lection d‚Äôune r√©gion Azure ‚Äúcompatible‚Äù##

En utilisant ce script, on attend un d√©lai al√©atoire (0/30/60/90/120s) pour ‚Äústagger‚Äù les d√©ploiements en classe. puis, on filtre les r√©gions o√π les providers n√©cessaires sont disponibles
et on choisit une r√©gion dans une liste pr√©f√©r√©e. En effet, Synapse n‚Äôest pas disponible partout. Certaines r√©gions peuvent √™tre satur√©es (quotas/stock) pendant une formation. On essaie alors avec ce bloc d'instructions de trouver une r√©gion d√©ployable. Ou bien on peut sp√©cifier le nom de la r√©gion souhait√©e comme argument √† l'instruction initiale qui lance le script en choisissant la r√©gion comme northeurope, westeurope, etc.

```
Start-Sleep -Seconds $delay
$locations = Get-AzLocation | Where-Object { ... }
```

## 7. Cr√©ation du Resource Group##

```
New-AzResourceGroup -Name $resourceGroupName -Location $Region | Out-Null
```

Avec cette action on va cr√©er le Ressource Group (RG). Ceci permet de regrouper toutes les ressources Synapse + Storage + compute.

## 8. D√©ploiement principal via un ARM template (setup.json)##

L'√©tape suivante consiste √† d√©ployer l‚Äôinfrastructure d√©crite dans le fichier ```setup.json``` en utilisant les instructions suivantes:

```
New-AzResourceGroupDeployment -ResourceGroupName $resourceGroupName `
  -TemplateFile "setup.json" `
  -Mode Complete `
  -workspaceName $synapseWorkspace `
  -dataLakeAccountName $dataLakeAccountName `
  -sparkPoolName $sparkPool `
  -sqlDatabaseName $sqlDatabaseName `
  -sqlUser $sqlUser `
  -sqlPassword $sqlPassword `
  -uniqueSuffix $suffix `
  -Force
```
Les ressources qui seront cr√©es dans cette infrastructure sont les suivantes:
- Azure Synapse Workspace: ```$synapseWorkspace = "synapse$suffix"```
- ADLS Gen2 / Storage Account ```(Data Lake): $dataLakeAccountName = "datalake$suffix"```
- Spark pool (Synapse Spark): ```$sparkPool = "spark$suffix"```
- Dedicated SQL pool ou base SQL dans Synapse: ```$sqlDatabaseName = "sql$suffix"```

## 9. (Optionnel / comment√©) Data Explorer (Kusto) pool##
```
#Stop-AzSynapseKustoPool ...
```

Cette instruction (comment√©) permet de stopper ou mettre en pause le pool Data Explorer. En effet, la partie Data Explorer (KQL) co√ªte et n‚Äôest pas toujours n√©cessaire pour cet exercice.

## 10. Attribution de r√¥les RBAC sur le Data Lake##
```
$subscriptionId = (Get-AzContext).Subscription.Id
$userName = ((az ad signed-in-user show) | ConvertFrom-JSON).UserPrincipalName
$id = (Get-AzADServicePrincipal -DisplayName $synapseWorkspace).id
New-AzRoleAssignment -Objectid $id -RoleDefinitionName "Storage ...ageAccounts/$dataLakeAccountName" ...
New-AzRoleAssignment -SignInName $userName -RoleDefinitionName "...ageAccounts/$dataLakeAccountName" ...
```

Avec ces instruction, on r√©cup√®re l‚Äôutilisateur connect√© (UserPrincipalName) et aussi le Service Principal / Managed Identity li√© √† Synapse (affich√© comme service principal). Par la suite, on attribue des r√¥les RBAC sur le storage account. ceci est essentiel pour que Synapse (pipelines, Spark, SQL) puisse lire/√©crire dans le Data Lake et pour que l‚Äôutilisateur puisse manipuler les donn√©es (upload, exploration, debug).

## 11. Cr√©ation du sch√©ma SQL via sqlcmd (setup.sql)##

```
sqlcmd -S "$synapseWorkspace.sql.azuresynapse.net" -U $sqlUser -P $sqlPassword -d $sqlDatabaseName -I -i setup.sql
```

Cette commande permet d'ex√©cuter le script ```setup.sql``` sur l‚Äôendpoint SQL de Synapse ```*.sql.azuresynapse.net``` en utilisant la base/pool cible : ```$sqlDatabaseName```. Ceci permet de mettre en place la couche ‚Äúserving / warehouse‚Äù : tables, sch√©mas, objets n√©cessaires au lab. Ceci rend le lab reproductible : pas de cr√©ation manuelle de tables.

## 12. Chargement des donn√©es dans SQL via bcp##

```
Get-ChildItem "./data/*.txt" -File | Foreach-Object {
    $file = $_.FullName
    $table = $_.Name.Replace(".txt","")
    bcp dbo.$table in $file -S "<server>" -U $sqlUser -P $sqlPassword -d $sqlDatabaseName -f <formatfile> ...
}

```
Pour chaque fichier ```.txt``` dans le r√©pertoire ```./data/```, on r√©cup√®re le nom de la table (m√™me nom que le fichier), on charge en bulk dans ```dbo.<table>``` via bcp en utilisant le un fichier .fmt (format) associ√©. Cette op√©ration illustre une approche ‚Äúbulk load‚Äù (ingestion rapide) vers une couche SQL analytique. En pratique, on ferait souvent COPY INTO / PolyBase / pipelines, mais bcp est tr√®s utile pour un setup rapide de lab.

## 13. Pause du SQL Pool (optimisation des co√ªts)##
```
Suspend-AzSynapseSqlPool -WorkspaceName $synapseWorkspace -Name $sqlDatabaseName -AsJob
```

Cette instruction met en pause le pool SQL (dedicated). Ceci est tr√®s important en formation : un Dedicated SQL Pool co√ªte tant qu‚Äôil est ‚Äúonline‚Äù. Ceci d√©montre √©galement une bonne pratique FinOps : arr√™ter quand non utilis√©.

## 14. Upload de fichiers CSV dans le Data Lake (container files)##
```
$storageAccount = Get-AzStorageAccount -ResourceGroupName $resourceGroupName -Name $dataLakeAccountName
$storageContext = $storageAccount.Context

Get-ChildItem "./files/*.csv" -File | Foreach-Object {
    $blobPath = "sales_data/$($_.Name)"
    Set-AzStorageBlobContent -File $_.FullName -Container "files" -Blob $blobPath -Context $storageContext
}
```

Avec ces instruction, on r√©cup√®re le storage account et son contexte. On charge ensuite tous les .csv vers le container ```files``` et on les place sous un ‚Äúr√©pertoire logique‚Äù : ```sales_data/```. Ceci constitue la zone ‚Äúlanding / raw‚Äù (m√™me si ici on charge directement dans un container). Ces fichiers seront par la suite utilis√©s pour alimenter les labs Spark / serverless SQL / pipelines (lecture depuis ADLS).

## 15. Cr√©ation d‚Äôun script KQL (comment√©)##
```
# New-AzSynapseKqlScript -WorkspaceName $synapseWorkspace -DefinitionFile "./files/ingest-data.kql"
```
Il s'agit d'une instruction comment√©e dans le script qu'on aurait pu utiliser pour d√©ployer un script KQL dans Synapse (Data Explorer). ceci nous permet d'initialiser du contenu KQL pour ingestion/queries.

Ce script PowerShell ex√©cut√© se pr√©sente comme un pipeline d‚Äôindustrialisation permettant de:
- Pr√©parer l‚Äôoutil (modules, providers)
- S√©curiser le contexte (bonne subscription)
- Param√©trer l‚Äôacc√®s SQL (login + password)
- D√©ployer l‚Äôinfrastructure (template ARM)
- Donner les droits (RBAC user + Synapse)
- Initialiser la couche warehouse (setup.sql)
- Charger des donn√©es (bcp)
- Optimiser les co√ªts (pause SQL pool)
- Pr√©parer la zone data lake (upload CSV)

## D√©couvrez Synapse Studio

Synapse Studio est un portail Web dans lequel vous pouvez g√©rer et utiliser les ressources de votre espace de travail Azure Synapse Analytics.
12. Une fois l'ex√©cution du script d'installation termin√©e, dans le portail Azure, acc√©dez au groupe de ressources dp203-xxxxxxx qu'il a cr√©√© et notez que ce groupe de ressources contient votre espace de travail Synapse.
 
13. Un compte de stockage pour votre lac de donn√©es, un pool Apache Spark (Apache Spark pool), un Pool d'explorateur de donn√©es (Data Explorer pool) et un pool SQL d√©di√© (Dedicated SQL pool).
 
14. S√©lectionnez votre espace de travail Synapse et, dans sa page Overview, dans la carte Open Synapse Studio, s√©lectionnez Open pour ouvrir Synapse Studio dans un nouvel onglet du navigateur. 
 
Synapse Studio est une interface Web que vous pouvez utiliser pour travailler avec votre espace de travail Synapse Analytics.
15. Sur le c√¥t√© gauche de Synapse Studio, utilisez l'ic√¥ne ‚Ä∫‚Ä∫ pour d√©velopper le menu - cela r√©v√®le les diff√©rentes pages de Synapse Studio que vous utiliserez pour g√©rer les ressources et effectuer des t√¢ches d'analyse de donn√©es, comme illustr√© ici :
 
16. Affichez la page Donn√©es (Data) et notez qu'il existe deux onglets contenant des sources de donn√©es :
‚îÄ	Un onglet Workspace contenant les bases de donn√©es d√©finies dans l'espace de travail (y compris les bases de donn√©es SQL d√©di√©es et les bases de donn√©es Data Explorer).
‚îÄ	Un onglet Linked contenant des sources de donn√©es li√©es √† l'espace de travail, y compris le stockage Azure Data Lake.
      
17. Affichez la page Develop. C'est ici que vous pouvez d√©finir des scripts et d'autres actifs utilis√©s pour d√©velopper des solutions de traitement de donn√©es.
18. Affichez la page Integrate. Vous utilisez cette page pour g√©rer les actifs d'ingestion et d'int√©gration de donn√©es ; tels que des pipelines pour transf√©rer et transformer des donn√©es entre des sources de donn√©es.
19. Affichez la page Monitor. C'est ici que vous pouvez observer les travaux de traitement de donn√©es pendant leur ex√©cution et afficher leur historique.
20. Affichez la page Manage. C'est l√† que vous g√©rez les pools, les runtimes et les autres actifs utilis√©s dans votre espace de travail Azure Synapse. Voir chacun des onglets de la section Pools Analytics et notez que votre espace de travail comprend les pools suivants :
‚îÄ	Pools SQL :
o	Int√©gr√© : un pool SQL sans serveur que vous pouvez utiliser √† la demande pour explorer ou traiter des donn√©es dans un lac de donn√©es √† l'aide de commandes SQL.
o	sqlxxxxxxx : un pool SQL d√©di√© qui h√©berge une base de donn√©es d'entrep√¥t de donn√©es relationnelles.
‚îÄ	Groupes Apache Spark :
o	sparkxxxxxxx : que vous pouvez utiliser √† la demande pour explorer ou traiter des donn√©es dans un lac de donn√©es en utilisant des langages de programmation comme Scala ou Python.
‚îÄ	Pools d'explorateur de donn√©es :
o	adxxxxxxxx : un pool d'explorateurs de donn√©es que vous pouvez utiliser pour analyser les donn√©es √† l'aide du langage de requ√™te Kusto (KQL).
 
Ing√©rer des donn√©es avec un pipeline
L'une des principales t√¢ches que vous pouvez effectuer avec Azure Synapse Analytics consiste √† d√©finir des pipelines qui transf√®rent (et si n√©cessaire, transforment) les donn√©es d'un large √©ventail de sources dans votre espace de travail pour analyse.
Utiliser la t√¢che Data Copy pour cr√©er un pipeline
21. Dans Synapse Studio, sur la page Home, s√©lectionnez Ingest pour ouvrir l'outil Copy Data.
 
22. Dans l'outil Copy Data, √† l'√©tape Propri√©t√©s, assurez-vous que Built-in copy task et Run once now sont s√©lectionn√©s, puis cliquez sur Suivant >.
  
23. √Ä l'√©tape Source, dans la sous-√©tape Dataset, s√©lectionnez les param√®tres suivants :
‚îÄ	Type de source : Tous
‚îÄ	Connexion : Cr√©ez une nouvelle connexion et dans le volet linked services qui s'affiche, sous l'onglet Fichier, s√©lectionnez HTTP. 
 
24. Continuez ensuite et cr√©ez une connexion √† un fichier de donn√©es en utilisant les param√®tres suivants :
‚îÄ	Nom : Products
‚îÄ	Description : liste de produits via http
‚îÄ	Connectez-vous via le runtime d'int√©gration : AutoResolveIntegrationRuntime
‚îÄ	URL de base : https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/01/adventureworks/products.csv
‚îÄ	Validation du certificat du serveur : Activer
‚îÄ	Type d'authentification : Anonyme
 
25. Avant de cr√©er la connexion on peut la v√©rifier en utilisant le bouton Tester la connexion.
26. Apr√®s avoir cr√©√© la connexion, sur la page Magasin de donn√©es source (Source data store), assurez-vous que les param√®tres suivants sont s√©lectionn√©s, puis s√©lectionnez Suivant > :
‚îÄ	URL relative : laisser vide
‚îÄ	M√©thode de requ√™te : GET
‚îÄ	En-t√™tes suppl√©mentaires : laissez vide
‚îÄ	Copie binaire : non s√©lectionn√©e
‚îÄ	D√©lai d'expiration de la demande : laissez le champ vide
‚îÄ	Nombre maximal de connexions simultan√©es : laissez le champ vide
27. √Ä l'√©tape Source, dans la sous-√©tape Configuration, s√©lectionnez Aper√ßu des donn√©es pour afficher un aper√ßu des donn√©es produit que votre pipeline va ing√©rer, puis fermez l'aper√ßu.
 
28. Apr√®s avoir pr√©-visualis√© les donn√©es, sur la page Param√®tres du format de fichier, assurez-vous que les param√®tres suivants sont s√©lectionn√©s, puis s√©lectionnez Suivant > :
‚îÄ	Format de fichier : texte d√©limit√©
‚îÄ	D√©limiteur de colonne : virgule (,)
‚îÄ	D√©limiteur de ligne : saut de ligne (\n)
‚îÄ	Premi√®re ligne comme en-t√™te : s√©lectionn√©
‚îÄ	Type de compression : Aucun
29. √Ä l'√©tape Destination, dans la sous-√©tape Dataset, s√©lectionnez les param√®tres suivants :
‚îÄ	Type de destination : Azure Data Lake Storage Gen 2
‚îÄ	Connexion : s√©lectionnez la connexion existante √† votre magasin de lac de donn√©es (celle-ci a √©t√© cr√©√©e pour vous lorsque vous avez cr√©√© l'espace de travail).
30. Apr√®s avoir s√©lectionn√© la connexion, √† l'√©tape Destination/Jeu de donn√©es, assurez-vous que les param√®tres suivants sont s√©lectionn√©s, puis s√©lectionnez Suivant > :
‚îÄ	Chemin du dossier : files/product_data
‚îÄ	Nom du fichier : produits.csv
‚îÄ	Comportement de copie : aucun
‚îÄ	Nombre maximal de connexions simultan√©es : laissez le champ vide
‚îÄ	Taille du bloc (Mo) : Laisser vide
31. √Ä l'√©tape Destination, √† la sous-√©tape Configuration, sur la page Param√®tres du format de fichier, assurez-vous que les propri√©t√©s suivantes sont s√©lectionn√©es. S√©lectionnez ensuite Suivant > :
‚îÄ	Format de fichier : texte d√©limit√©
‚îÄ	D√©limiteur de colonne : virgule (,)
‚îÄ	D√©limiteur de ligne : saut de ligne (\n)
‚îÄ	Ajouter un en-t√™te au fichier : S√©lectionn√©
‚îÄ	Type de compression : Aucun
‚îÄ	Nombre maximum de lignes par fichier : laissez vide
‚îÄ	Pr√©fixe du nom de fichier : laissez vide
32. √Ä l'√©tape Param√®tres, saisissez les param√®tres suivants, puis cliquez sur Suivant > :
‚îÄ	Nom de la t√¢che : copier des produits
‚îÄ	Description de la t√¢che : Copier les donn√©es des produits
‚îÄ	Tol√©rance aux pannes : laisser vide
‚îÄ	Activer la journalisation : non s√©lectionn√©
‚îÄ	Activer la mise en sc√®ne : non s√©lectionn√©
33. √Ä l'√©tape R√©viser et terminer, √† la sous-√©tape R√©viser, lisez le r√©sum√©, puis cliquez sur Suivant >.
34. √Ä l'√©tape D√©ploiement, attendez que le pipeline soit d√©ploy√©, puis cliquez sur Terminer.
 
35. Dans Synapse Studio, s√©lectionnez la page Monitor, et dans l'onglet Pipeline runs, attendez que le pipeline Copy products se termine avec un statut R√©ussi (vous pouvez utiliser le bouton ‚Üª Refresh sur la page Pipeline runs pour actualiser le statut).
36. Affichez la page Int√©grer et v√©rifiez qu'elle contient d√©sormais un pipeline nomm√© Copier les produits.
 
Afficher les donn√©es ing√©r√©es
37. Sur la page Donn√©es, s√©lectionnez l'onglet Linked et d√©veloppez la hi√©rarchie Azure Data Lake Storage Fichiers produit jusqu'√† ce que vous voyiez le stockage des fichiers pour votre espace de travail Synapse. S√©lectionnez ensuite le stockage de fichiers pour v√©rifier qu'un dossier nomm√© product_data contenant un fichier nomm√© products.csv a √©t√© copi√© √† cet emplacement, comme illustr√© ici :
 
38. Cliquez avec le bouton droit sur le fichier de donn√©es products.csv et s√©lectionnez Aper√ßu pour visualiser les donn√©es ing√©r√©es. Fermez ensuite l'aper√ßu.
 
Utiliser un pool SQL sans serveur pour analyser les donn√©es
Maintenant que vous avez ing√©r√© des donn√©es dans votre espace de travail, vous pouvez utiliser Synapse Analytics pour les interroger et les analyser. L'une des fa√ßons les plus courantes d'interroger des donn√©es consiste √† utiliser SQL, et dans Synapse Analytics, vous pouvez utiliser un pool SQL sans serveur pour ex√©cuter du code SQL sur des donn√©es dans un lac de donn√©es.
39. Dans Synapse Studio, cliquez avec le bouton droit sur le fichier products.csv dans le stockage de fichiers de votre espace de travail Synapse, pointez sur Nouveau script SQL et s√©lectionnez S√©lectionner les 100 premi√®res lignes.
 
40. Dans le volet SQL Script 1 qui s'ouvre, passez en revue le code SQL qui a √©t√© g√©n√©r√©, qui devrait ressembler √† ceci :
-- This is auto-generated code
 SELECT
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØTOP‚ÄØ100‚ÄØ*
 FROM
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØOPENROWSET(
‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØBULK‚ÄØ'https://datalakexxxxxxx.dfs.core.windows.net/files/product_data/products.csv',
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØFORMAT‚ÄØ=‚ÄØ'CSV',
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØPARSER_VERSION='2.0'
     )‚ÄØAS‚ÄØ[result]
41. Ce code ouvre un ensemble de lignes √† partir du fichier texte que vous avez import√© et r√©cup√®re les 100 premi√®res lignes de donn√©es.
42. Dans la liste Connect to (Se connecter √†), assurez-vous que Built-in est s√©lectionn√© - cela repr√©sente le pool SQL int√©gr√© qui a √©t√© cr√©√© avec votre espace de travail.
 
43. Dans la barre d'outils, utilisez le bouton ‚ñ∑ Ex√©cuter pour ex√©cuter le code SQL et examinez les r√©sultats, qui devraient ressembler √† ceci :
 
44. Notez que les r√©sultats se composent de quatre colonnes nomm√©es C1, C2, C3 et C4 ; et que la premi√®re ligne des r√©sultats contient les noms des champs de donn√©es. Pour r√©soudre ce probl√®me, ajoutez un param√®tre HEADER_ROW = TRUE √† la fonction OPENROWSET comme illustr√© ici (en rempla√ßant datalakexxxxxxx par le nom de votre compte de stockage de lac de donn√©es), puis relancez la requ√™te :
SELECT
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØTOP‚ÄØ100‚ÄØ*
 FROM
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØOPENROWSET(
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØBULK‚ÄØ'https://datalakexxxxxxx.dfs.core.windows.net/files/product_data/products.csv',
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØFORMAT‚ÄØ=‚ÄØ'CSV',
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØPARSER_VERSION='2.0',
         HEADER_ROW = TRUE
     )‚ÄØAS‚ÄØ[result]
45. Maintenant, les r√©sultats ressemblent √† ceci :
 
46. Modifiez la requ√™te comme suit (en rempla√ßant datalakexxxxxxx par le nom de votre compte de stockage de lac de donn√©es) :
SELECT
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØCategory, COUNT(*) AS ProductCount
 FROM
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØOPENROWSET(
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØBULK‚ÄØ'https://datalakexxxxxxx.dfs.core.windows.net/files/product_data/products.csv',
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØFORMAT‚ÄØ=‚ÄØ'CSV',
 ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØPARSER_VERSION='2.0',
         HEADER_ROW = TRUE
     )‚ÄØAS‚ÄØ[result]
 GROUP BY Category;
47. Ex√©cutez la requ√™te modifi√©e, qui devrait renvoyer un jeu de r√©sultats contenant le nombre de produits dans chaque cat√©gorie, comme ceci :
 
48. Dans le volet Propri√©t√©s de SQL Script 1, remplacez le nom par Count Products by Category. 
49. Ensuite, dans la barre d'outils, s√©lectionnez Publier pour enregistrer le script.
50. Fermez le volet de script Count Products by Category.
51. Dans Synapse Studio, s√©lectionnez la page D√©velopper et notez que votre script SQL Count Products by Category publi√© y a √©t√© enregistr√©.
52. S√©lectionnez le script SQL Count Products by Category pour le rouvrir. Assurez-vous ensuite que le script est connect√© au pool SQL int√©gr√© et ex√©cutez-le pour r√©cup√©rer le nombre de produits.
53. Dans le volet R√©sultats, s√©lectionnez la vue Graphique, puis s√©lectionnez les param√®tres suivants pour le graphique :
‚îÄ	Type de graphique : Colonne
‚îÄ	Colonne Cat√©gorie : Cat√©gorie
‚îÄ	Colonnes de l√©gende (s√©rie) : ProductCount
‚îÄ	Position de la l√©gende : bas ‚Äì centre
‚îÄ	√âtiquette de l√©gende (s√©rie) : Laisser vide
‚îÄ	L√©gende (s√©rie) valeur minimale : Laisser vide
‚îÄ	L√©gende (s√©rie) maximum : Laisser vide
‚îÄ	Libell√© de la cat√©gorie : laisser vide
     Le graphique r√©sultant devrait ressembler √† ceci :
 
Utiliser un pool Spark pour analyser les donn√©es
Alors que SQL est un langage courant pour interroger des ensembles de donn√©es structur√©s, de nombreux analystes de donn√©es trouvent des langages comme Python utiles pour explorer et pr√©parer les donn√©es pour l'analyse. Dans Azure Synapse Analytics, vous pouvez ex√©cuter du code Python (et autre) dans un pool Spark ; qui utilise un moteur de traitement de donn√©es distribu√© bas√© sur Apache Spark.
54. Dans Synapse Studio, si l'onglet Fichiers que vous avez ouvert pr√©c√©demment contenant le fichier products.csv n'est plus ouvert, sur la page Donn√©es, parcourez le dossier product_data. 
55. Cliquez ensuite avec le bouton droit sur products.csv, pointez sur Nouveau bloc-notes et s√©lectionnez Charger dans DataFrame.
 
56. Dans le volet Bloc-notes 1 qui s'ouvre, dans la liste Attacher √†, s√©lectionnez le pool Spark xxxxxxxx et assurez-vous que la langue est d√©finie sur PySpark (Python).
 
57. Passez en revue le code dans la premi√®re (et unique) cellule du bloc-notes, qui devrait ressembler √† ceci :
%%pyspark
df = spark.read.load('abfss://files@datalakeex24n6q.dfs.core.windows.net/product_data/produits.csv', format='csv'
## If‚ÄØheader‚ÄØexists‚ÄØuncomment‚ÄØline‚ÄØbelow
##, header=True
)
display(df.limit(10))
58. Utilisez l'ic√¥ne ‚ñ∑ √† gauche pour l'ex√©cuter, et attendez les r√©sultats. 
59. La premi√®re fois que vous ex√©cutez une cellule dans un bloc-notes, le pool Spark est d√©marr√©. Cela peut donc prendre environ une minute pour renvoyer des r√©sultats.
60. Finalement, les r√©sultats devraient appara√Ætre sous la cellule, et ils devraient ressembler √† ceci :
 
61. D√©-commentez la ligne, header=True (car le fichier products.csv a les en-t√™tes de colonne sur la premi√®re ligne).
62. R√©-ex√©cutez la cellule et v√©rifiez que les r√©sultats ressemblent √† ceci :
 
63. Notez que la r√©ex√©cution de la cellule prend moins de temps, car le pool Spark est d√©j√† d√©marr√©.
64. Sous les r√©sultats, utilisez l'ic√¥ne Ôºã Code pour ajouter une nouvelle cellule de code au bloc-notes.
65. Dans la nouvelle cellule de code vide, ajoutez le code suivant :
      df_counts = df.groupby(df.Category).count()
      display(df_counts)
66. Ex√©cutez la nouvelle cellule de code en cliquant sur son ic√¥ne ‚ñ∑ et examinez les r√©sultats, qui devraient ressembler √† ceci :
 
67. Dans la sortie des r√©sultats de la cellule, s√©lectionnez-la vue Graphique. Le graphique r√©sultant devrait ressembler √† ceci :
 
68. Si elle n'est pas d√©j√† visible, affichez la page Propri√©t√©s en s√©lectionnant le bouton Propri√©t√©s √† l'extr√©mit√© droite de la barre d'outils. 
69. Ensuite, dans le volet Propri√©t√©s, modifiez le nom du bloc-notes en Explorer les produits et utilisez le bouton Publier de la barre d'outils pour l'enregistrer.
70. Fermez le volet du bloc-notes et arr√™tez la session Spark lorsque vous y √™tes invit√©. Affichez ensuite la page D√©velopper pour v√©rifier que le bloc-notes a √©t√© enregistr√©.
Utiliser un pool SQL d√©di√© pour interroger un entrep√¥t de donn√©es
Jusqu'√† pr√©sent, vous avez vu quelques techniques d'exploration et de traitement de donn√©es bas√©es sur des fichiers dans un lac de donn√©es. Dans de nombreux cas, une solution d'analyse d'entreprise utilise un lac de donn√©es pour stocker et pr√©parer des donn√©es non structur√©es qui peuvent ensuite √™tre charg√©es dans un entrep√¥t de donn√©es relationnelles pour prendre en charge les charges de travail de Business Intelligence (BI). Dans Azure Synapse Analytics, ces entrep√¥ts de donn√©es peuvent √™tre impl√©ment√©s dans un pool SQL d√©di√©.
71. Dans Synapse Studio, sur la page Manage, dans la section Pools SQL, s√©lectionnez la ligne du pool SQL d√©di√© sqlxxxxxxx
72. Utilisez son ic√¥ne ‚ñ∑ pour le reprendre.
 
73. Attendez que le pool SQL d√©marre. Cela peut prendre quelques minutes. Utilisez le bouton ‚Üª Actualiser pour v√©rifier p√©riodiquement son √©tat. L'√©tat s'affichera comme Online lorsqu'il sera pr√™t.
74. Lorsque le pool SQL a d√©marr√©, s√©lectionnez la page Data.
75. Dans l'onglet Espace de travail, d√©veloppez les bases de donn√©es SQL et v√©rifiez que sqlxxxxxxx est r√©pertori√© (utilisez l'ic√¥ne ‚Üª en haut √† gauche de la page pour actualiser la vue si n√©cessaire).
 
76. D√©veloppez la base de donn√©es sqlxxxxxxx et son dossier Tables, puis dans le menu ‚Ä¶ de la table FactInternetSales, pointez sur Nouveau script SQL et s√©lectionnez S√©lectionner les 100 premi√®res lignes.
 
77. Examinez les r√©sultats de la requ√™te, qui affichent les 100 premi√®res transactions de vente dans le tableau. Ces donn√©es ont √©t√© charg√©es dans la base de donn√©es par le script d'installation et sont stock√©es en permanence dans la base de donn√©es associ√©e au pool SQL d√©di√©.
78. Remplacez la requ√™te SQL par le code suivant :
     SELECT d.CalendarYear, d.MonthNumberOfYear, d.EnglishMonthName,
        p.EnglishProductName AS Product, SUM(o.OrderQuantity) AS UnitsSold
 FROM dbo.FactInternetSales AS o
 JOIN dbo.DimDate AS d ON o.OrderDateKey = d.DateKey
 JOIN dbo.DimProduct AS p ON o.ProductKey = p.ProductKey
 GROUP BY d.CalendarYear, d.MonthNumberOfYear, d.EnglishMonthName, p.EnglishProductName
 ORDER BY d.MonthNumberOfYear
79. Utilisez le bouton ‚ñ∑ Ex√©cuter pour ex√©cuter la requ√™te modifi√©e, qui renvoie la quantit√© de chaque produit vendu par ann√©e et par mois.
 
80. Si elle n'est pas d√©j√† visible, affichez la page Propri√©t√©s en s√©lectionnant le bouton Propri√©t√©s √† l'extr√©mit√© droite de la barre d'outils. 
81. Ensuite, dans le volet Propri√©t√©s, modifiez le nom de la requ√™te en Aggregate product sales.
82. Utilisez le bouton Publier de la barre d'outils pour l'enregistrer.
83. Fermez le volet de requ√™te, puis affichez la page Develop pour v√©rifier que le script SQL a √©t√© enregistr√©.
84. Sur la page Manage, s√©lectionnez la ligne du pool SQL d√©di√© sqlxxxxxxx et utilisez son ic√¥ne ‚ùö‚ùö pour la mettre en pause.
Explorer les donn√©es avec un pool d'explorateur de donn√©es
Azure Synapse Data Explorer fournit un environnement d'ex√©cution que vous pouvez utiliser pour stocker et interroger des donn√©es √† l'aide du langage de requ√™te Kusto (KQL). Kusto est optimis√© pour les donn√©es qui incluent un composant de s√©rie chronologique, telles que les donn√©es en temps r√©el provenant de fichiers journaux ou d'appareils IoT.
Cr√©er une base de donn√©es Data Explorer et ing√©rer des donn√©es dans une table
85. Dans Synapse Studio, sur la page Manage, dans le pool Data Explorer pools, s√©lectionnez la ligne du pool adxxxxxxxx, puis utilisez son ic√¥ne ‚ñ∑ pour la reprendre.
86. Attendez que le pool d√©marre. Cela peut prendre un certain temps. Utilisez le bouton ‚Üª Actualiser pour v√©rifier p√©riodiquement son √©tat. Le statut s'affichera comme en ligne lorsqu'il sera pr√™t.
87. Lorsque le pool de l'explorateur de donn√©es a d√©marr√©, affichez la page Data; et dans l'onglet Espace de travail, d√©veloppez Bases de donn√©es de l'explorateur de donn√©es et v√©rifiez que adxxxxxxxx est r√©pertori√© (utilisez l'ic√¥ne ‚Üª en haut √† gauche de la page pour actualiser la vue si n√©cessaire)
88. Dans le volet Data, utilisez l'ic√¥ne Ôºã pour cr√©er une nouvelle base de donn√©es Data Explorer dans le pool adxxxxxxxx avec le nom sales-data.
 
89. Dans Synapse Studio, attendez que la base de donn√©es soit cr√©√©e (une notification s'affichera).
90. Passez √† la page Develop et, dans la liste des scripts KQL, s√©lectionnez ingest-data. Lorsque le script s'ouvre, notez qu'il contient deux instructions :
‚îÄ	Une instruction .create table pour cr√©er une table nomm√©e sales.
‚îÄ	Une instruction .ingest into table pour charger des donn√©es dans la table √† partir d'une source HTTP.
91. Dans le volet ingest-data, dans la liste Connect to, s√©lectionnez votre pool adxxxxxxxx, et dans la liste Database, s√©lectionnez sales-data.
 
92. Dans le script, mettez en surbrillance l'instruction .create table, puis dans la barre d'outils, utilisez le bouton ‚ñ∑ Run pour ex√©cuter le code s√©lectionn√©, ce qui cr√©e une table nomm√©e sales.
93. Une fois la table cr√©√©e, mettez en surbrillance l'instruction .ingest into table et utilisez le bouton ‚ñ∑ Run pour l'ex√©cuter et ing√©rer les donn√©es dans la table.
94. Dans cet exemple, vous avez import√© une tr√®s petite quantit√© de donn√©es de lot √† partir d'un fichier, ce qui convient aux fins de cet exercice. En r√©alit√©, vous pouvez utiliser Data Explorer pour analyser des volumes de donn√©es beaucoup plus importants ; y compris les donn√©es en temps r√©el d'une source de diffusion telle qu'Azure Event Hubs.
Utiliser le langage de requ√™te Kusto pour interroger la table
95. Revenez √† la page Data et dans le menu ‚Ä¶ de la base de donn√©es des donn√©es sales-data, s√©lectionnez Actualiser.
 
96. D√©veloppez le dossier Tables de la base de donn√©es des donn√©es sales-data. 
97. Ensuite, dans le menu ‚Ä¶ de la table des ventes, s√©lectionnez Nouveau script KQL > Prendre 1000 lignes.
 
98. Passez en revue la requ√™te g√©n√©r√©e et ses r√©sultats. La requ√™te doit contenir le code suivant :
     sales
 | take 1000
99. Les r√©sultats de la requ√™te contiennent les 1 000 premi√®res lignes de donn√©es.
 
100. Modifiez la requ√™te comme suit :
sales
 | where Item == 'Road-250 Black, 48'
101.  Utilisez le bouton ‚ñ∑ Ex√©cuter pour ex√©cuter la requ√™te. 
102. Examinez ensuite les r√©sultats, qui ne doivent contenir que les lignes des bons de commande pour le produit Road-250 Black, 48.
103. Modifiez la requ√™te comme suit :
sales
 | where Item == 'Road-250 Black, 48'
 | where datetime_part('year', OrderDate) > 2020
104.  Ex√©cutez la requ√™te et examinez les r√©sultats, qui ne doivent contenir que des commandes client pour Road-250 Black, 48 effectu√©es apr√®s 2020.
105. Modifiez la requ√™te comme suit :
sales
 | where OrderDate between (datetime(2020-01-01 00:00:00) .. datetime(2020-12-31 23:59:59))
 | summarize TotalNetRevenue = sum(UnitPrice) by Item
 | sort by Item asc
106. Ex√©cutez la requ√™te et examinez les r√©sultats, qui doivent contenir le revenu net total pour chaque produit entre le 1er janvier et le 31 d√©cembre 2020, par ordre croissant de nom de produit.
107. Si elle n'est pas d√©j√† visible, affichez la page Propri√©t√©s en s√©lectionnant le bouton Propri√©t√©s √† l'extr√©mit√© droite de la barre d'outils. 
108. Ensuite, dans le volet Propri√©t√©s, modifiez le nom de la requ√™te en Explore sales data de vente et utilisez le bouton Publier de la barre d'outils pour l'enregistrer.
109. Fermez le volet de requ√™te, puis affichez la page Develop pour v√©rifier que le script KQL a √©t√© enregistr√©.
110. Sur la page Manage, s√©lectionnez la ligne du pool adxxxxxxxx Data Explorer et utilisez son ic√¥ne ‚ùö‚ùö pour la mettre en pause.
Supprimer des ressources Azure
Maintenant que vous avez fini d'explorer Azure Synapse Analytics, vous devez supprimer les ressources que vous avez cr√©√©es pour √©viter des co√ªts Azure inutiles.
111. Fermez l'onglet du navigateur Synapse Studio et revenez au portail Azure.
112. Sur le portail Azure, sur la page d'accueil, s√©lectionnez Groupes de ressources.
113. S√©lectionnez le groupe de ressources dp203-xxxxxxx pour votre espace de travail Synapse Analytics (pas le groupe de ressources g√©r√©es) et v√©rifiez qu'il contient l'espace de travail Synapse, le compte de stockage, le pool SQL, le pool Data Explorer et le pool Spark pour votre espace de travail.
114. En haut de la page Pr√©sentation de votre groupe de ressources, s√©lectionnez Supprimer le groupe de ressources.
115. Entrez le nom du groupe de ressources dp203-xxxxxxx pour confirmer que vous souhaitez le supprimer, puis s√©lectionnez Supprimer.
116. Apr√®s quelques minutes, votre groupe de ressources d'espace de travail Azure Synapse et le groupe de ressources d'espace de travail g√©r√© qui lui est associ√© seront supprim√©s.


## Pr√©requis

Avant de commencer les exercices, assurez-vous d‚Äôavoir :

- Un abonnement Azure actif (id√©alement avec des cr√©dits de formation ou un abonnement payant)
- Les droits n√©cessaires pour cr√©er des ressources dans une ressource group (Contributor au minimum)
- Un ordinateur avec :
  - Un navigateur web r√©cent (Edge ou Chrome recommand√©)
  - [Azure Data Studio](https://learn.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio) (optionnel mais recommand√© pour certains labs)
  - [Azure Storage Explorer](https://azure.microsoft.com/en-us/features/storage-explorer/) (optionnel)

> **Note** : Les labs utilisent souvent un environnement temporaire (ex. : sandbox Microsoft Learn ou d√©ploiement via script). Si vous travaillez en autonomie, vous devrez cr√©er votre propre ressource Synapse Workspace.

## Structure du repository

Chaque dossier de lab contient :
- Un fichier `README.md` avec les instructions d√©taill√©es de l‚Äôexercice
- Les notebooks Synapse (.ipynb)
- Les scripts SQL
- Les pipelines (fichiers .json si export√©s)
- Les jeux de donn√©es d‚Äôexemple

## Comment d√©marrer les labs

### Option 1 : Pendant la formation (recommand√©)
Le formateur vous fournira :
- L‚Äôacc√®s √† un environnement pr√©-d√©ploy√© (Synapse Workspace, donn√©es charg√©es, etc.)
- Les instructions sp√©cifiques pour chaque lab

Suivez simplement les README dans chaque dossier de lab.

### Option 2 : En autonomie apr√®s la formation

1. Cr√©ez un Azure Synapse Workspace dans votre abonnement
2. T√©l√©chargez ou clonez ce repository
3. Ouvrez le workspace Synapse Studio
4. Importez les notebooks, pipelines et scripts SQL depuis les dossiers correspondants
5. Chargez les donn√©es d‚Äôexemple depuis le dossier `resources/`
6. Suivez les instructions dans le `README.md` de chaque lab

> **Astuce** : Certains labs incluent des scripts de d√©ploiement automatis√© (ARM templates ou PowerShell). V√©rifiez dans le dossier `scripts/` si disponible.

## Nettoyage des ressources

Pour √©viter des co√ªts inutiles :
- Supprimez le resource group contenant votre Synapse Workspace √† la fin des exercices
- Ou mettez en pause les pools Spark / SQL d√©di√©s

## Support et questions

- Si vous rencontrez un probl√®me avec les instructions ou les fichiers ‚Üí ouvrez une **Issue** sur ce repository
- Pour des questions sur Azure Synapse Analytics ‚Üí consultez la [documentation officielle Microsoft](https://learn.microsoft.com/en-us/azure/synapse-analytics/)

## Contributeurs

- [Votre Nom] ‚Äì Formateur principal / Cr√©ateur du contenu

---

Merci d‚Äôavoir suivi cette formation !  
Bonne pratique avec Azure Synapse Analytics üöÄ
